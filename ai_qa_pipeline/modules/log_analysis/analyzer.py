"""
AI Log Analyzer
===============

–ê–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤ –∏ –æ—Ç—á–µ—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é LLM –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è root cause failures.
"""

import json
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, field
from pathlib import Path

from ..test_generation.llm_client import LLMClient, LLMProvider


@dataclass
class FailurePattern:
    """–ü–∞—Ç—Ç–µ—Ä–Ω failure –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–π –≤ –ª–æ–≥–∞—Ö"""
    pattern_type: str  # timeout, assertion, network, element_not_found, etc.
    frequency: int
    affected_tests: List[str]
    root_cause: str
    suggestion: str


@dataclass
class AnalysisResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥–æ–≤"""
    total_failures: int
    unique_patterns: int
    patterns: List[FailurePattern] = field(default_factory=list)
    root_causes: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)
    summary: str = ""

    def to_dict(self) -> Dict[str, Any]:
        return {
            "total_failures": self.total_failures,
            "unique_patterns": self.unique_patterns,
            "patterns": [
                {
                    "type": p.pattern_type,
                    "frequency": p.frequency,
                    "affected_tests": p.affected_tests,
                    "root_cause": p.root_cause,
                    "suggestion": p.suggestion
                }
                for p in self.patterns
            ],
            "root_causes": self.root_causes,
            "recommendations": self.recommendations,
            "summary": self.summary
        }


class LogAnalyzer:
    """
    AI Log Analyzer

    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç:
    - Test failures –∏ –∏—Ö –ø—Ä–∏—á–∏–Ω—ã
    - –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –ø–∞—Ç—Ç–µ—Ä–Ω—ã –æ—à–∏–±–æ–∫
    - Network/timeout issues
    - Element locator problems
    - Assertion failures
    - Environment issues
    """

    def __init__(
        self,
        llm_provider: LLMProvider = LLMProvider.OPENAI,
        model: Optional[str] = None,
        api_key: Optional[str] = None
    ):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è analyzer"""
        self.llm = LLMClient(
            provider=llm_provider,
            model=model,
            api_key=api_key,
            temperature=0.3
        )

    def analyze_test_results(
        self,
        test_results_json: str
    ) -> AnalysisResult:
        """
        –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–æ–≤ –∏–∑ JSON

        Args:
            test_results_json: –ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏

        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞
        """
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        with open(test_results_json, 'r') as f:
            results = json.load(f)

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ failures
        failures = []
        for test in results.get("tests", []):
            if test.get("outcome") in ["failed", "error"]:
                failures.append({
                    "test_id": test.get("nodeid"),
                    "error": test.get("call", {}).get("longrepr", ""),
                    "duration": test.get("duration", 0)
                })

        if not failures:
            return AnalysisResult(
                total_failures=0,
                unique_patterns=0,
                summary="No failures detected"
            )

        # AI –∞–Ω–∞–ª–∏–∑ failures
        prompt = self._create_analysis_prompt(failures)
        analysis = self.llm.generate_json(prompt)

        return self._parse_analysis(analysis, len(failures))

    def analyze_logs(
        self,
        log_file: str
    ) -> AnalysisResult:
        """–ê–Ω–∞–ª–∏–∑ –ª–æ–≥-—Ñ–∞–π–ª–∞"""
        with open(log_file, 'r') as f:
            logs = f.read()

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –∏–∑ –ª–æ–≥–æ–≤
        errors = self._extract_errors_from_logs(logs)

        if not errors:
            return AnalysisResult(
                total_failures=0,
                unique_patterns=0,
                summary="No errors found in logs"
            )

        # AI –∞–Ω–∞–ª–∏–∑
        prompt = f"""
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–µ –æ—à–∏–±–∫–∏ –∏–∑ –ª–æ–≥–æ–≤ —Ç–µ—Å—Ç–æ–≤:

{json.dumps(errors, indent=2)}

–û–ø—Ä–µ–¥–µ–ª–∏:
1. –ü–∞—Ç—Ç–µ—Ä–Ω—ã –æ—à–∏–±–æ–∫ (timeout, assertion, network, element not found, etc.)
2. Root cause –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
3. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—é

–í–µ—Ä–Ω–∏ JSON:
{{
  "patterns": [
    {{
      "type": "timeout",
      "frequency": 5,
      "affected_tests": ["test1", "test2"],
      "root_cause": "Slow page load",
      "suggestion": "Increase timeout or optimize page"
    }}
  ],
  "root_causes": ["Root cause 1", "Root cause 2"],
  "recommendations": ["Recommendation 1", "Recommendation 2"],
  "summary": "Overall summary"
}}
"""

        analysis = self.llm.generate_json(prompt)
        return self._parse_analysis(analysis, len(errors))

    def _create_analysis_prompt(self, failures: List[Dict[str, Any]]) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ prompt –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        prompt = f"""
–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π QA-–∏–Ω–∂–µ–Ω–µ—Ä. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–µ failures –∏–∑ –∞–≤—Ç–æ—Ç–µ—Å—Ç–æ–≤:

{json.dumps(failures, indent=2)}

–¢–≤–æ—è –∑–∞–¥–∞—á–∞:
1. –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã –æ—à–∏–±–æ–∫ (–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö failures)
2. –ù–∞–π—Ç–∏ root cause –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
3. –ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å —Ä–µ—à–µ–Ω–∏—è

–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤:
- timeout: —ç–ª–µ–º–µ–Ω—Ç –Ω–µ –ø–æ—è–≤–∏–ª—Å—è –≤–æ–≤—Ä–µ–º—è
- assertion: –æ–∂–∏–¥–∞–µ–º–æ–µ != —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ
- element_not_found: –ª–æ–∫–∞—Ç–æ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω
- network: –ø—Ä–æ–±–ª–µ–º—ã —Å —Å–µ—Ç—å—é/API
- environment: –ø—Ä–æ–±–ª–µ–º—ã –æ–∫—Ä—É–∂–µ–Ω–∏—è
- data: –ø—Ä–æ–±–ª–µ–º—ã —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏

–í–µ—Ä–Ω–∏ JSON:
{{
  "patterns": [
    {{
      "type": "timeout",
      "frequency": 3,
      "affected_tests": ["test_login", "test_checkout"],
      "root_cause": "Page load too slow, default timeout insufficient",
      "suggestion": "Increase page.goto timeout to 60s or optimize page performance"
    }}
  ],
  "root_causes": [
    "Main root cause 1",
    "Main root cause 2"
  ],
  "recommendations": [
    "Actionable recommendation 1",
    "Actionable recommendation 2"
  ],
  "summary": "Overall summary of failures and main issues"
}}

–í–ê–ñ–ù–û: –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º, –¥–∞–≤–∞–π actionable recommendations.
"""
        return prompt

    def _parse_analysis(
        self,
        analysis: Dict[str, Any],
        total_failures: int
    ) -> AnalysisResult:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞"""
        patterns = []

        for pattern_data in analysis.get("patterns", []):
            patterns.append(FailurePattern(
                pattern_type=pattern_data.get("type", "unknown"),
                frequency=pattern_data.get("frequency", 1),
                affected_tests=pattern_data.get("affected_tests", []),
                root_cause=pattern_data.get("root_cause", ""),
                suggestion=pattern_data.get("suggestion", "")
            ))

        return AnalysisResult(
            total_failures=total_failures,
            unique_patterns=len(patterns),
            patterns=patterns,
            root_causes=analysis.get("root_causes", []),
            recommendations=analysis.get("recommendations", []),
            summary=analysis.get("summary", "")
        )

    def _extract_errors_from_logs(self, logs: str) -> List[Dict[str, str]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ª–æ–≥–æ–≤"""
        errors = []
        lines = logs.split("\n")

        for i, line in enumerate(lines):
            if "ERROR" in line or "FAILED" in line or "Exception" in line:
                # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç (3 —Å—Ç—Ä–æ–∫–∏ –¥–æ –∏ –ø–æ—Å–ª–µ)
                context_start = max(0, i - 3)
                context_end = min(len(lines), i + 4)
                context = "\n".join(lines[context_start:context_end])

                errors.append({
                    "line_number": i + 1,
                    "error_line": line,
                    "context": context
                })

        return errors

    def generate_report(self, result: AnalysisResult) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è markdown –æ—Ç—á–µ—Ç–∞"""
        report = f"""# Test Failure Analysis Report

## Summary
{result.summary}

**Total Failures:** {result.total_failures}
**Unique Patterns:** {result.unique_patterns}

---

## Failure Patterns

"""

        for pattern in result.patterns:
            report += f"""### {pattern.pattern_type.upper()} ({pattern.frequency} occurrences)

**Root Cause:** {pattern.root_cause}

**Affected Tests:**
{chr(10).join(f'- {test}' for test in pattern.affected_tests)}

**üí° Suggestion:**
{pattern.suggestion}

---

"""

        if result.root_causes:
            report += "\n## Main Root Causes\n\n"
            for i, cause in enumerate(result.root_causes, 1):
                report += f"{i}. {cause}\n"

        if result.recommendations:
            report += "\n## Recommendations\n\n"
            for i, rec in enumerate(result.recommendations, 1):
                report += f"{i}. {rec}\n"

        return report
